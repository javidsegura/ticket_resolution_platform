apiVersion: 1

# 1. Define where to send the alerts
contactPoints:
  - orgId: 1
    name: slack-alerts
    receivers:
      - uid: slack-notifier
        type: slack
        settings:
          url: $SLACK_WEBHOOK_URL
          title: "ðŸš¨ Application Alert"
          message: |
            **Alert:** {{ .Status }}
            **Summary:** {{ .CommonAnnotations.summary }}
            **Description:** {{ .CommonAnnotations.description }}

# 2. Define the routing (Send everything to Slack)
policies:
  - orgId: 1
    receiver: slack-alerts
    group_by: ['alertname']
    group_wait: 10s
    group_interval: 30s
    repeat_interval: 5m

# 3. Define the actual Alert Rules
groups:
  - orgId: 1
    name: "FastAPI Critical Alerts"
    folder: "FastAPI Observability"
    interval: 1m
    rules:
      # RULE: High Error Rate (5xx)
      # Logic: Checks if the rate of 5xx errors is greater than 0 over the last 1 minute
      - uid: high_5xx_error_rate
        title: "High HTTP 5xx Error Rate"
        condition: C
        for: 1m # How long the threshold must be breached before firing
        labels:
          severity: critical
        annotations:
          summary: "High rate of 500 errors detected"
          description: "The 5xx error rate is {{ $values.B.Value }} req/s, which is above the threshold of 0."
        data:
          - refId: A
            relativeTimeRange: { from: 300, to: 0 }
            datasourceUid: "PBFA97CFB590B2093" # Matches the UID in datasources.yml
            model:
              # This is the Prometheus Query
              expr: sum(rate(http_requests_total{status=~"5.*"}[1m]))
              intervalMs: 1000
              maxDataPoints: 43200
              refId: A
          - refId: B
            # Reduce the time series to a single number (Mean)
            datasourceUid: "__expr__"
            model:
              conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                    reducer:
                      params: []
                      type: avg
                  query:
                    params: []
              expression: A
              reducer: mean
              refId: B
              type: reduce
          - refId: C
            # The Threshold Check (B > 0)
            datasourceUid: "__expr__"
            model:
              conditions:
                - evaluator:
                    params: [0] # Threshold value
                    type: gt
                  operator:
                    type: and
                    reducer:
                      params: []
                      type: avg
                  query:
                    params: []
              expression: B
              type: threshold

      # RULE: High Latency (P99 > 1 second)
      - uid: high_latency_p99
        title: "High Latency (P99)"
        condition: C
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "P99 Latency is high"
          description: "99% of requests are taking longer than 1s."
        data:
          - refId: A
            relativeTimeRange: { from: 300, to: 0 }
            datasourceUid: "PBFA97CFB590B2093"
            model:
              # Calculates P99 latency
              expr: histogram_quantile(0.99, sum(rate(http_request_duration_highr_seconds_bucket[1m])) by (le))
              refId: A
          - refId: B
            datasourceUid: "__expr__"
            model:
              expression: A
              reducer: mean
              refId: B
              type: reduce
          - refId: C
            datasourceUid: "__expr__"
            model:
              conditions:
                - evaluator:
                    params: [1] # Threshold: 1 second
                    type: gt
                  operator:
                    type: and
                    reducer:
                      type: avg
              expression: B
              type: threshold
